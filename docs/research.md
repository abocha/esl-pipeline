# ChatGPT 

## Monorepo Package Structure and Naming

The ESL pipeline is organized as a PNPM monorepo under a packages/ directory, with each major component in its own package. All packages use the naming convention @esl-pipeline/<packagename> and serve distinct roles. Key packages include:

- md-extractor - Library of functions to parse an ESL homework Markdown and extract structured parts like frontmatter, study text, answer key, etc.
- md-validator - A CLI tool to validate the structure and content of the ESL Markdown file (ensuring required sections, frontmatter fields, etc.).
- notion-importer - Handles creating Notion pages from the Markdown, using Notion's SDK to upload content and metadata.
- notion-colorizer - Stub CLI tool intended to apply color presets to headings/toggles in Notion pages (currently reads a presets JSON and returns a placeholder result).
- tts-elevenlabs - Tool to generate text-to-speech audio for the study text section using the ElevenLabs API (currently implemented as a stub that creates a dummy MP3 and hash for testing).
- storage-uploader - Tool to upload generated files (like audio) to storage (e.g. S3), returning a URL (currently also a stub that constructs an S3 URL without real upload).
- notion-add-audio - Uses the Notion API to embed an audio player under the study text in the Notion page (given an audio URL).
- orchestrator - The high-level CLI that ties all the above tools together in a pipeline. It runs validation, import, colorizing, TTS generation, upload, and audio embedding in sequence to produce a new "assignment" in Notion.

This structured layout makes each concern modular. The packages share a common version and are linked via workspace references (the orchestrator depends on the others via workspace:\* versions). The naming scheme clearly indicates the function (e.g. md-\* for Markdown utilities, notion-\* for Notion-related tasks, etc.), which helps maintain clarity as the project grows.

## Shared Types and Utilities

Rather than a single "shared" package, common types and utilities are defined in context and imported across packages as needed. For example, the frontmatter schema for homework metadata is defined in md-validator using Zod (fields like title, student, level, etc.). Other packages use a compatible shape - e.g. notion-importer defines a FrontmatterShape type matching those fields and leverages the md-extractor to parse frontmatter accordingly. The orchestrator defines an AssignmentManifest type to record the outcome of a pipeline run (MD hash, Notion page ID/URL, audio file info, preset used, timestamp). This manifest is written to a JSON file alongside the markdown for reference. These types (frontmatter, manifest, etc.) are exported by their packages so that other tools can reuse them. The decision to colocate schemas/types with their primary logic keeps each package self-contained, while still allowing import by others (e.g. orchestrator imports the other packages and could utilize their types via TypeScript).

# CLI Design and Tooling 

Commander.js is used to build the CLI interface for tools that need robust flag parsing. For example, md-validator sets up a Commander program named md-validate with a file argument and a -strict flag. This provides a consistent user experience (usage info, help, etc.). The orchestrator likely uses Commander as well for its higher-level command (e.g. a new command with numerous options for file path, student, preset, etc.), leveraging Commander's option parsing to map to an internal flags object .

Chalk is used for colored console output, making CLI feedback more readable. The validator CLI prints a green check mark and success message if validation passes, yellow warnings for non-critical issues, and red errors for failures. For instance, after validation md-validator will output passed in green or list Errors in red and Warnings in yellow, along with any frontmatter metadata echoed in a muted gray. This color-coding of messages is a structural choice to improve UX for developers running the tools.

Although not yet implemented in the current code, the scaffolding references Ora (a spinner library) as part of the CLI toolkit. The intention is to use Ora's spinner to indicate progress during longer-running steps (like network calls to Notion or audio generation). At present, no ora usage is in the package dependencies (the CLI outputs are mostly instantaneous or stubbed), but future updates may integrate spinners for steps in the orchestrator pipeline. The combination of Commander for command/flag parsing, Chalk for color, and Ora for progress feedback represents a common CLI scaffolding pattern that Codex set up to ensure the tools are user-friendly and extensible.

## Notion API Integration (2025-09-03)

Notion integration is handled via the official @notionhq/client library. The project uses v5.3.0 of the SDK and explicitly targets the 2025-09-03 Notion API version which introduces data source support. When creating the Notion client, the code sets the notionVersion: '2025-09-03' and authenticates with a token from environment variables. Using this recent API version allows the pipeline to create pages in Notion's AI data source context rather than a traditional database. For example, when creating a new homework page, the importer does not use a database parent ID but instead supplies a parent: \{ data_source: \{ id: ... \} \} in the page creation request. This means the content goes into a special data source (such as an AI workspace or an inline database designated as a data source) rather than a static DB - a structural decision aligned with Notion's latest features.

To resolve where to create the page, the importer can take either a database name/ID or a data source name/ID from CLI flags. It uses a helper to find the correct data source: if a direct --datasource-id is provided, it fetches that data source and infers its parent database. Otherwise, if a database is specified (by ID or name), it retrieves that database's available data sources. If multiple data sources exist for one database, the CLI flag --data-source <name> can select one; if none or ambiguous, it errors with guidance. This logic, built into resolveDataSourceId, ensures the CLI is flexible in finding the correct Notion target without hardcoding IDs.

Additionally, the pipeline supports linking the new page to a student. The environment variable STUDENTS_DB_ID is expected to point to a Students data source (or database) in Notion. The importer uses this to find a student's page ID by name. It will first query that data source for a page whose "Name" property matches the provided student name. If found, the student's page ID is added as a relation property ('Student') on the new homework page. If not found (or STUDENTS_DB_ID isn't provided), it falls back to a global search for a page with that name. This approach externalizes student configuration to Notion (maintaining a Students database there) rather than requiring a local config file.

In summary, Codex chose to use Notion's newest API capabilities to future-proof the tool. The Notion SDK v5.3.0 with data source support is central, and all Notion operations (page creation, queries) are done through this client. API credentials (the integration token) and target IDs are pulled from environment and flags rather than hardcoded, making the tool adaptable to different workspaces.

# Validation Schema and JSON Schema Strategy 

The pipeline relies on Zod for schema validation of Markdown frontmatter and structure. In the mdvalidator', a Zod schema defines the expected frontmatter fields and types: title, student, level, topic (all required strings), an input_type which must be either "generate" or "authentic", and optionally speaker_labels (an array of speaker name strings) for dialogues. This schema is used to parse the YAML frontmatter via gray-matter and validate it. Any missing or invalid fields produce clear error messages - e.g. "Front matter: title - Required" if the title is blank. By using Zod's. safeParse, the validator collects all schema violations at once and reports them as errors.

In addition to frontmatter, the validator enforces structural rules on the Markdown content using markdown AST parsing (Unified/Remark). It expects exactly 9 top-level sections (H2 headings) matching a specific sequence of titles (the numbered "Mission" sections). If a section is missing or out of order, an error is raised. It also checks for required special blocks: the ::: study - text block for the main study content, and toggle blocks for "Answer Key" and "Teacher's Follow-up Plan". If these markers are missing, errors are added (e.g. "Missing ::: study-text block."). Further content checks are applied: for dialogues, each line inside the study-text must be in the format "[Speaker]: ..." with the speaker name matching one of the allowed labels from frontmatter (errors are logged for unknown or malformed speaker lines). For monologues (no speaker labels), it emits a warning if the text is too short (less than 3 paragraphs or lines). There are also soft checks on exercise sections, e.g. if a "Controlled Practice" section exists, it counts the items and warns if not in the 8-10 range , and checks "Comprehension Check" has at least 2 items. These validation rules, while specific, codify the expected structure of assignments.

Although not yet outputting a formal JSON Schema file, the use of Zod means the frontmatter schema could be exported as a JSON Schema in the future. This would be useful if other systems (or even Notion formulas/integrations) need the schema. For now, the focus is on runtime validation and user feedback. The validator returns a structured result ('ok', errors[], 'warnings[]', and a meta object for parsed frontmatter) which the CLI consumes to print messages. This design allows programmatic use too - e.g. the importer can call the validator and decide whether to proceed.

In summary, Codex's scaffolding emphasizes strict validation of content before it goes into Notion. The combination of Zod schemas for metadata and markdown parsing for content provides both hard guarantees (required fields/sections) and guidance (length or count recommendations via warnings). As development continues, this validated schema can serve as a contract for content format, and could be leveraged for JSON Schema export or documentation.

# Testing Approach with Vitest and Fixtures 

The project is set up with Vitest as the test runner for all packages (each package.json includes vitest in devDependencies and has test scripts). The codebase includes a mix of unit tests and fixture-based tests to ensure each tool works as expected. For example, the md-extractor package has tests that feed in a sample markdown string and assert that each extractor function returns the right output. In these tests, a multiline template string represents a fake homework markdown (with frontmatter and sections), and calls to extractFrontmatter, extractStudyText, etc., are verified (e.g. confirming the title and student from frontmatter, or that study text type is "dialogue" when two speakers are present).

The md-validator package takes a similar approach but also provides fixture files under a fixtures/ directory for realism. For instance, there is an ok.md fixture representing a correctly structured assignment. The tests load this file and run the validator to ensure it passes without errors and yields expected metadata. There will likely also be fixture cases for known failure modes (e.g. missing sections or markers) to test that the validator catches them and returns the proper error messages. Using actual markdown files as fixtures makes the tests more maintainable and closer to real usage, which is important as the content format evolves.

The other packages (notion-importer, orchestrator, etc.) have lighter tests at this stage, often focusing on stubbed functionality. For example, the ElevenLabs TTS module's test confirms that calling buildStudyTextMp3 produces an .mp3 file path and a hash without actually needing an API call - since the implementation is stubbed, it ensures the stub behaves consistently (returning a dummy file and a hash string of expected length). Similarly, the storage uploader's test would check that it returns a well-formed S3 URL when given a file path, using a fake bucket name.

Overall, the testing strategy is to use Vitest with a mix of unit tests and integration-like tests using sample data. This ensures the scaffolding is sound: content parsing, validation, and stubbed external interactions all work in isolation. As development continues (e.g. adding real API calls), these tests and fixtures will help catch regressions and validate that the pipeline's assumptions (Markdown format, etc.) hold true.

## Composition of Markdown Tools (Validator \& Extractor)

Codex structured the pipeline so that markdown parsing and validation are decoupled into reusable tools, and then composed in the higher-level commands. The md-validator and md-extractor are used across the pipeline to ensure every step deals with correct, structured data:

- Validator in Importer: Before trying to create a Notion page, the notion-importer runs the markdown through the validator. Rather than duplicating validation logic, the importer simply calls the CLI of md-validator as a subprocess (md-validator <file> --strict ) to enforce a "fail fast" if the content is invalid. This means the import will abort with clear errors rather than creating a bad page. (In the future, they could call validateMarkdownFile directly via an API instead of spawning a process, but the current setup uses the CLI for simplicity.)

- Extractor in Importer: Once validation passes, the importer uses md-extractor to pull out the frontmatter from the markdown text. The frontmatter fields (title, topics, student name) become inputs for the Notion page's properties. For example, the extracted title becomes the Notion page title, and if a student name is present (or provided via CLI), it is resolved to a Notion relation as described earlier. The extractor could also be used to get the main content in a structured way, but in practice the importer directly converts the markdown to Notion blocks (via a custom mdToBlocks function) rather than using extractStudyText - this is because the entire markdown (minus frontmatter) needs to be preserved as Notion blocks, not broken into pieces except metadata.
- Extractor in TTS: The tts-elevenlabs module uses md-extractor indirectly to generate audio. It provides a function buildStudyTextMp3(mdPath, \{ voiceMapPath, ... \}) that reads the markdown and likely uses extractStudyText internally. This yields the cleaned study text lines and type (dialogue/monologue), which the TTS logic can iterate over to generate spoken audio for each line or the whole passage. The orchestrator calls buildStudyTextMp3 with the markdown path and a voice mapping, and receives back a result containing the output file path and a hash. Additionally, a helper hashStudyText(mdPath) computes a hash of the study text content (used to generate a unique page identifier and to store in the manifest). By centralizing this in the TTS package (which in turn likely uses the extractor), consistency is ensured - the same exact subset of content is hashed and voiced.
- Validator \& Extractor in Orchestrator: The orchestrator, as the coordinator, ensures all these tools run in order. It first triggers validation and import. Currently, the orchestrator calls the importer (which itself invokes the validator internally), but one could imagine orchestrator explicitly running md-validator then notion-importer. In either case, those steps occur at the start of the pipeline. Only after a page is created (import) does it proceed to optional steps like colorize, TTS, etc. The orchestrator doesn't need to re-validate at that point because the content is already validated upstream. It does, however, reuse extraction indirectly when doing the TTS and manifest: for example, it uses the hashStudyText function (which uses extractor logic) to compute a short unique ID for the page and to include the full MD hash in the manifest for change detection.

This composition shows a clear separation of concerns: the Markdown validator ensures content integrity, the extractor provides structured views of the markdown content, and higher-level tools consume those. Each piece can be developed and tested in isolation (as we saw with their tests), and issues are caught early (invalid MD never reaches Notion). Codex's design makes the pipeline robust and paves the way for future enhancements (for instance, replacing the CLI subprocess call with direct function calls once everything is in TypeScript, or extending the extractor/validator for new markdown patterns).

# Configuration: Color Presets, Student Configs, Voice Maps 

The pipeline introduces configurable aspects to tailor the output for different scenarios:

- Color Presets: To enforce consistent color-coding in Notion pages (for headings, toggles, etc.), the system allows a preset to be applied. Presets are defined in a JSON file (by default expected at /configs/presets.json) which maps preset names to specific color assignments. The

notion-colorizer CLI accepts a --preset <name> and an optional --presets-path to specify a custom JSON file. In the current scaffold, applyHeadingPreset(pageId, presetName) is a stub that returns a result without changing Notion content (just indicating zero headings updated). However, the structure is in place to later load the presets file, call the Notion API to color headings accordingly, and output the count of changes. The orchestrator integrates this by calling applyHeadingPreset if a preset flag is provided, right after importing the page. This step is logged as "colorize" in the pipeline steps and the manifest records which preset was applied. By organizing presets in a JSON, it's easy to add or modify color themes without code changes, and multiple preset names can be supported.

- Student Configs: As mentioned, the link between a homework and a student is configured via Notion rather than a local file. The environment variable STUDENTS_DB_ID (or data source ID) essentially acts as the "student config." It tells the system where to look up student pages. In practice, this means all student names must exist as page entries in the Notion database/data source referenced. This design offloads student management to Notion (which is logical, since that's where the student pages live) and avoids maintaining a separate mapping of student name to ID locally. If needed, the system could be extended to use a local config (e.g. a JSON mapping names to IDs) but the current approach is to rely on the Notion API to resolve names. Aside from that, most student-specific behavior (like choosing voices for TTS or customizing assignments) could also be configured via frontmatter in the markdown (for example, the frontmatter could include a field that selects a voice preset per student). In summary, "student configs" in the current architecture are minimal - primarily the Notion reference for linking keeping the pipeline generic.
- Voice Maps: The TTS generation allows customization of which synthetic voice to use for each "speaker" in a dialogue. This is handled through a voice map JSON file. By default the orchestrator (and the tts-elevenlabs CLI) looks for a file named voices.json (or a path given via --voice-map). This JSON is expected to map speaker labels (like "Alex", "Mara" or even generic "A", "B") to specific voice IDs or settings from ElevenLabs. The TTS module will read this map to know which voice to apply to each dialogue line. For example, if the homework has [Alex]: Hi and [Mara]: Hello in the study text, the voice map might map "Alex" -> a male voice ID, "Mara" -> a female voice ID. The voice map can also include a default or a mapping for a narrator in monologues. This external JSON approach means adding or changing voices doesn't require code changes - just edit the JSON to use new voice IDs or adjust who is speaking. In the current implementation, since everything is stubbed, the voice map isn't deeply used except to pass along the file path. But the structure is ready: buildStudyTextMp3 takes the voiceMapPath and when fully implemented will load that file, then call the ElevenLabs API for each chunk of text accordingly. By organizing voices in a JSON, one could even maintain multiple voice profiles for different contexts (e.g. different languages or accents) and choose which map to use via the CLI flag.

All these configs - presets JSON, voice map JSON, and the student database ID - live outside the core code, which is an intentional design for flexibility. The CLI surfaces them as flags or env vars rather than hardcoding. This allows users or future developers to extend the system (add new presets, support new voices or students) by configuration rather than changing logic. It's also aligned with best practices: keep secrets (like API keys, S3 buckets) in env vars, and user preferences (like colors or voice choices) in data files.

# CLI Flags and Environment Variables 

Codex's scaffolding exposes many options via CLI flags, making the pipeline highly configurable from the command line. The orchestrator CLI (esl-orchestrator) in particular has a rich set of flags, corresponding to the NewAssignmentFlags interface :

- --md <path> : (Required) Path to the markdown file containing the assignment.
- --student <name> : Student name to link the assignment to (if omitted, can be derived from frontmatter if present).
- --preset <name> : Name of a color preset to apply to the Notion page's headings (triggers the colorizer step if provided).
- --with-tts : Boolean flag to generate text-to-speech audio for the study text. If set, expects a voices.json (or custom path via --voices ) in the working directory to map voices, and will produce an MP3.
- --voices <file> : Path to the voice map JSON (defaults to voices. json in current directory if not given).
- --out <dir> : Output directory for generated audio files (if TTS is on). Defaults to the markdown file's directory if not specified .
- --upload <backend> : If provided (e.g. --upload s3 ), after generating audio the file will be uploaded to that storage backend. Currently only 's3' is recognized .
- --dry-run : If set, the importer will not actually create a Notion page or upload files. Instead it will output a preview of what would happen (e.g. show the prepared properties and number of blocks). The orchestrator also respects --dry-run by not uploading publicly when set.
- --force : Used in the audio embedding step - if an audio link already exists in the Notion page, --force will replace it. Otherwise, by default the notion-add-audio step might skip adding if one is present (to avoid duplicates).
- --db <name> / --db-id <id> : Identify the Notion database to use for import. --db allows specifying the database by name (the code will search the workspace for a matching database name 18 ), while --db-id can directly provide the UUID. From this, the appropriate data source is resolved as described earlier.
- --data-source <name> / --data-source-id <id> : Identify the specific data source (Notion AI dataset) under the given database to target. This is optional; if not provided, and the database has exactly one data source, that one is used. If multiple and none specified, the CLI will error asking for a name or ID .

Each of the individual tools (like md-validator, notion-importer, etc.) also have their own flags, often a subset of the above relevant to their function. For instance, md-validator supports strict to treat warnings as errors. notion-importer likely mirrors some orchestrator flags for database and data source (to know where to create the page) and has --student and --dry-run flags as well - this is implied by how the importer's ImportOptions are structured and how it calls resolveDataSourceId and uses opts.student. The TTS CLI (tts-elevenlabs ) as shown requires --md and --voice-map (and has optional --out and --preview). The storage uploader CLI uses --file and --public flags. Having these as separate tools is useful for testing and flexibility (one can run them standalone), but the orchestrator is the primary interface that gathers all needed options for the full pipeline.

On the environment variable side, the project relies on env vars for sensitive or environmentspecific config: - NOTION_TOKEN - The API token for the Notion integration. This must be present; otherwise createNotionClient() throws an error. Keeping this in env (not in code or flags) is important for security. - STUDENTS_DB_ID - The identifier (most likely a data source ID) of the Students database in Notion, to resolve student names. If not set, linking students by name will only use the slower global search. - S3_BUCKET and S3_PREFIX - The target S3 bucket name and key prefix for file uploads. The storage uploader uses these to construct the file URL. In development or tests, if these aren't set, it falls back to a dummy stub-bucket and default audio/assignments prefix, effectively generating a fake URL. In production, you'd set these to your actual bucket and prefix. (AWS credentials for upload are assumed to be configured in the environment/instance; since the actual AWS SDK call isn't implemented yet, no explicit AWS_ACCESS_KEY is referenced, but those would be expected to be configured for real uploads.) - ELEVENLABS_API_KEY - Not explicitly shown in code yet, but presumably will be needed when integrating the real ElevenLabs API for TTS. This would be stored in env and read by the TTS module to authenticate requests. Currently, the absence of an actual API call in code means this isn't used, but it's an anticipated env config for the future. - Other environment configs could include logging settings or feature flags, but none are present in the scaffold. Most other runtime choices are via CLI flags as described.

The CLI flag parsing and env usage together give a great deal of control. A user running the pipeline can provide just a markdown file and let defaults handle the rest, or specify everything (which student, which preset, where to upload, etc.). This design allows further development to add new flags or env configs easily. For example, adding a --azure-tts option or a new storage backend could be done by introducing new flag values and corresponding implementation in the respective package, without altering the core structure. The separation between flags and env also follows a convention: use flags for functional behavior of this run, use env for secrets or global config. This way, one doesn't accidentally log or expose a secret by passing it as a flag, and things like tokens remain outside source control.

In conclusion, the Codex-generated architecture for the ESL pipeline is modular, extensible, and clearly scaffolded for growth. The monorepo's structured packages and shared types enforce separation of concerns, while the CLI design and config files enable flexibility. All critical decisions (Notion data source usage, strict content schema, stubbed external integrations) have been made with an eye toward future development - ensuring that as the stubs are replaced with real API calls and new features are added, the foundation is solid and maintainable. Each part of the system knows its role, and the orchestrator ties them together in a cohesive flow. With this understanding, further development can proceed confidently, building on the existing scaffolding to implement the remaining functionality and refine the tools.